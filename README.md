# Order-Up ğŸš€

> **Kafka Streams first, Flink-ready by design**

A production-grade data pipeline where the **processing engine is a replaceable module**.

You are not "choosing Streams over Flink" â€” you are **deferring the engine choice** behind stable contracts.

---

## ğŸ¯ The Core Idea

Build a pipeline where switching from Kafka Streams to Flink (or any other engine) requires **changing only one component**, not rewriting the entire system.

### Why This Matters

- **Start simple**: Kafka Streams for initial delivery
- **Scale when needed**: Swap to Flink for complex CEP, ML, or massive state
- **Minimize risk**: Same input/output contracts mean the rest of the pipeline is unaffected
- **Test both**: Run both engines simultaneously against the same data to validate behavior

---

## ğŸ— Architecture

```
Python Producer
   â†“
Kafka: orders.events.raw (Avro, immutable)
   â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Processing Engine (pluggable)      â”‚
â”‚  â”œâ”€ Kafka Streams (Java) â† Day 1    â”‚
â”‚  â””â”€ Flink (later)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   â†“
Kafka: orders.orders.curated (stable output schema)
   â†“
Python Sink
   â”œâ”€ Snowflake (fact table, idempotent)
   â””â”€ Optional S3/Iceberg (raw archive)
```

### Key Design Rule

> **ğŸ‘‰ Only the engine changes. Everything else stays the same.**

---

## ğŸ“ Topic & Schema Strategy

### Raw Events (never rewritten)

**Topic**: `orders.events.raw`

**Schema** (Avro envelope):
```
event_id         : string (UUID)
event_type       : enum (ORDER_CREATED, ORDER_UPDATED, ...)
event_ts         : timestamp (event time)
order_id         : string (key)
payload          : union (nested record by event type)
schema_version   : int
```

- **Append-only forever**
- Immutable event log
- Complete audit trail

### Curated Output (engine contract)

**Topic**: `orders.orders.curated`

**Schema**: One row per order (or per order window)

- Designed for Snowflake consumption
- **This is the engine contract**
- Both Kafka Streams and Flink **must produce this exact shape**

---

## ğŸ“¦ Repository Structure

```
order-up/
â”œâ”€â”€ docker-compose.yml          # Kafka (KRaft) + Schema Registry
â”œâ”€â”€ producer-python/            # Python event producer
â”œâ”€â”€ engine-streams-java/        # Kafka Streams processor
â”œâ”€â”€ engine-flink/               # Flink processor (placeholder)
â”œâ”€â”€ sink-python/                # Python consumer â†’ Snowflake
â”œâ”€â”€ schemas/                    # Avro schemas (source of truth)
â””â”€â”€ README.md                   # This file
```

---

## ğŸš€ Quick Start

### 1. Start Infrastructure

```bash
docker-compose up -d
```

This launches:
- **Kafka** (KRaft mode, no Zookeeper) on `localhost:9092`
- **Schema Registry** on `localhost:8081`
- **Kafka UI** on `localhost:8080` (optional, for debugging)

### 2. Verify Services

```bash
# Check Kafka
docker exec -it kafka kafka-topics --bootstrap-server localhost:9092 --list

# Check Schema Registry
curl http://localhost:8081/subjects
```

### 3. Run Components

```bash
# Producer (generates events)
cd producer-python && python main.py

# Kafka Streams Engine
cd engine-streams-java && ./gradlew run

# Sink (to Snowflake)
cd sink-python && python main.py
```

---

## ğŸ”„ Engine Swap Strategy

### Phase 1: Kafka Streams (Day 1)
- Implement Kafka Streams processor in Java
- Read from `orders.events.raw`
- Write to `orders.orders.curated`
- Validate end-to-end flow

### Phase 2: Flink (Later)
- Implement Flink job with **identical logic**
- Use **same input topic** (`orders.events.raw`)
- Use **same output topic** (`orders.orders.curated`)
- Use **same Avro schemas**

### Phase 3: Validation
- Run both engines in parallel
- Compare output for consistency
- Switch traffic when confident

### Phase 4: Cutover
- Replace Streams with Flink
- **Zero changes** to Producer or Sink
- Monitor and validate

---

## ğŸ§ª Testing the Swap

Because the contract is stable, you can:

1. **Shadow Mode**: Run Flink alongside Streams, writing to different topic for comparison
2. **A/B Test**: Route 10% of traffic to Flink, 90% to Streams
3. **Full Cutover**: Replace Streams with Flink entirely

The producer and sink **never know the difference**.

---

## ğŸ“Š Monitoring

Each engine exposes metrics in its native format:
- **Kafka Streams**: JMX metrics
- **Flink**: Flink REST API + Prometheus

The pipeline itself is monitored via:
- Topic lag (consumer group lag)
- Schema evolution (Schema Registry compatibility)
- End-to-end latency (event_ts â†’ sink write)

---

## ğŸ“ Design Principles

1. **Immutable Events**: Raw events are never updated or deleted
2. **Schema Evolution**: Use Avro with backward/forward compatibility
3. **Idempotent Sinks**: Snowflake writes use upsert/merge logic
4. **Event Time**: Processing based on `event_ts`, not ingestion time
5. **Exactly-Once Semantics**: Both engines configured for EOS
6. **Stable Contracts**: Input and output schemas are the API

---

## ğŸ”® Future Enhancements

- [ ] Add Flink implementation
- [ ] Implement S3/Iceberg archive sink
- [ ] Add dead-letter queue for malformed events
- [ ] Implement schema evolution testing
- [ ] Add metrics dashboard (Grafana)
- [ ] Kubernetes deployment manifests

---

## ğŸ“ Notes

- **KRaft Mode**: Kafka runs without Zookeeper (production-ready as of Kafka 3.3+)
- **Schema Registry**: Stores Avro schemas for producer/consumer compatibility
- **Exactly-Once**: Requires transactional producers and consumers
- **Idempotency**: Essential for reliable pipeline behavior

---

## ğŸ“š Resources

- [Kafka Streams Documentation](https://kafka.apache.org/documentation/streams/)
- [Apache Flink Documentation](https://flink.apache.org/)
- [Avro Schema Evolution](https://docs.confluent.io/platform/current/schema-registry/avro.html)
- [Snowflake COPY INTO](https://docs.snowflake.com/en/sql-reference/sql/copy-into-table.html)

---

**Built with â¤ï¸ for pluggable, production-grade streaming pipelines**

